{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from functools import reduce\n",
    "import random\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20d688839e8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        self.W = {'val': np.random.standard_normal((in_channel,out_channel)), 'grad': 0}\n",
    "        self.b = {'val': np.random.randn(out_channel), 'grad': 0}\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = np.dot(X, self.W['val']) + self.b['val']\n",
    "        self.cache = X\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        X = self.cache\n",
    "        dX = np.dot(dout, self.W['val'].T).reshape(X.shape)\n",
    "        self.W['grad'] = np.dot(X.reshape(X.shape[0], np.prod(X.shape[1:])).T, dout)\n",
    "        self.b['grad'] = np.sum(dout, axis=0)\n",
    "#         self.update()\n",
    "        return dX\n",
    "    \n",
    "    def update(self, lr=0.001):\n",
    "        self.W['val'] -= lr*self.W['grad']\n",
    "        self.b['val'] -= lr*self.b['grad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d():\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, init_params=False):\n",
    "        \"\"\"\n",
    "        :param in_channels: (int) the input channel\n",
    "        :param out_channels: (int) the output channel\n",
    "        :param kernel_size: (int) the kernel size\n",
    "        :param stride: (int) the stirde\n",
    "        \"\"\"\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.input_h = None\n",
    "        self.input_w = None\n",
    "        self.out_h = None\n",
    "        self.out_w = None\n",
    "\n",
    "        self.weight_gradient = 0\n",
    "        self.bias_gradient = 0\n",
    "\n",
    "        self.init_params = init_params\n",
    "\n",
    "        self.W = {'val': np.random.randn(self.in_channels, self.out_channels, self.kernel_size, self.kernel_size), 'grad': 0}\n",
    "        self.b = {'val': np.random.randn(self.out_channels, 1), 'grad': 0}\n",
    "\n",
    "\n",
    "        # 输入图像的batch_size=N，默认为1\n",
    "        self.batch_size = 1\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: (N, C_in, H_in, W_in) 通道*高度*宽度\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        self.input_map = x\n",
    "\n",
    "        if not self.init_params:\n",
    "            self.init_params = True\n",
    "            weights_scale = math.sqrt(reduce(lambda x, y: x * y, self.input_map.shape) / self.out_channels)\n",
    "\n",
    "            self.W['val'] = np.random.standard_normal(\n",
    "                size=(self.in_channels, self.out_channels, self.kernel_size, self.kernel_size)) / weights_scale\n",
    "            self.b['val'] = np.random.standard_normal(size=(self.out_channels, 1)) / weights_scale\n",
    "\n",
    "        self.batch_size, _, self.input_h, self.input_w = x.shape\n",
    "\n",
    "        self.out_h = int((self.input_h-self.kernel_size)/self.stride + 1)\n",
    "        self.out_w = int((self.input_w-self.kernel_size)/self.stride + 1)\n",
    "        # print('out_h:', self.out_h)\n",
    "        # print('out_w:', self.out_w)\n",
    "\n",
    "        # 图像转换为矩阵，N*(H*W)*(C*K*K)\n",
    "        self.col_images = []\n",
    "\n",
    "        weight_col = self.W['val'].reshape(self.out_channels, -1)\n",
    "        # N * C_out * H_out * W_out\n",
    "        conv_out = np.zeros((self.batch_size, self.out_channels, self.out_h, self.out_w))\n",
    "        for batch_i in range(self.batch_size):\n",
    "            # 输入的第i个图像C_in*H_in*W_in\n",
    "            image_batch_i = x[batch_i, :]\n",
    "            image_batch_i_col = self.im2col(image_batch_i, self.kernel_size, self.stride)\n",
    "\n",
    "            self.col_images.append(image_batch_i_col)\n",
    "            # print(image_batch_i_col.shape)\n",
    "            # print(weight_col.shape)\n",
    "            conv_out[batch_i] = np.reshape(np.dot(weight_col, np.transpose(image_batch_i_col))+self.b['val'], (self.out_channels, self.out_h, self.out_w))\n",
    "\n",
    "        self.col_images = np.array(self.col_images)\n",
    "\n",
    "        return conv_out\n",
    "\n",
    "    # 计算梯度过程中同时将误差反向传播计算出来，根据当前误差返回上一误差\n",
    "    def backward(self, error):\n",
    "        self.error = error\n",
    "        error_col = self.error.reshape(self.batch_size, self.out_channels, -1)\n",
    "        # print('self.col_images.shape:', self.col_images.shape)\n",
    "        # print('error_col.shape:', error_col.shape)\n",
    "        # print('error.shape:', error.shape)\n",
    "\n",
    "        for batch_i in range(self.batch_size):\n",
    "            self.W['grad'] += np.dot(error_col[batch_i], self.col_images[batch_i]).reshape(self.W['val'].shape)\n",
    "        # 将对应的维度相加，需要将N和最后求和\n",
    "        self.b['grad'] += np.sum(error_col, axis=(0, 2)).reshape(self.b['val'].shape)\n",
    "        # 反向传播计算上一层error\n",
    "\n",
    "        error_pad = np.pad(self.error, ((0, 0), (0, 0), (self.kernel_size - 1, self.kernel_size - 1), (self.kernel_size - 1, self.kernel_size - 1)), 'constant', constant_values=0)\n",
    "        # print('error_pad.shape:', error_pad.shape)\n",
    "        # print('error:', error)\n",
    "        # print('error_pad:', error_pad)\n",
    "\n",
    "        weight_flip = self.W['val'][:, :, ::-1, ::-1]\n",
    "        weight_flip = np.swapaxes(weight_flip, 0, 1)\n",
    "        weight_flip_col = weight_flip.reshape(self.in_channels, -1)\n",
    "        # print('weight_flip_col.shape:', weight_flip_col.shape)\n",
    "\n",
    "\n",
    "        next_error = np.zeros((self.batch_size, self.in_channels, self.input_h, self.input_w))\n",
    "        for batch_i in range(self.batch_size):\n",
    "            # 输入的第i个图像C_in*H_in*W_in\n",
    "            error_pad_image_batch_i = error_pad[batch_i, :]\n",
    "            error_pad_image_batch_i_col = self.im2col(error_pad_image_batch_i, self.kernel_size, self.stride)\n",
    "            # print('error_pad_image_batch_i_col.shape:', error_pad_image_batch_i_col.shape)\n",
    "            next_error[batch_i] = np.reshape(np.dot(weight_flip_col, np.transpose(error_pad_image_batch_i_col)), (self.in_channels, self.input_h, self.input_w))\n",
    "\n",
    "\n",
    "        return next_error\n",
    "    \n",
    "    def im2col(self, image, ksize, stride):\n",
    "        # image is a 3d tensor([channel, height, width])\n",
    "        image_col = []\n",
    "        for i in range(0, image.shape[1] - ksize + 1, stride):\n",
    "            for j in range(0, image.shape[2] - ksize + 1, stride):\n",
    "                col = image[:,i:i + ksize, j:j + ksize].reshape([-1])\n",
    "                image_col.append(col)\n",
    "        image_col = np.array(image_col)\n",
    "        return image_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = (kernel_size,kernel_size,in_channels,out_channels)\n",
    "        self.stride = stride\n",
    "        self.cache = 0\n",
    "        \n",
    "        self.W = {'val': np.random.standard_normal((self.kernel_size)), 'grad': np.zeros(self.kernel_size)}\n",
    "        self.b = {'val': np.random.standard_normal(out_channels), 'grad': np.zeros(out_channels)}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        (N,Cin,H,W) = x.shape\n",
    "        self.input_shape = x.shape\n",
    "        H_ = int((H - self.kernel_size[0]) / self.stride + 1)\n",
    "        W_ = int((W - self.kernel_size[0]) / self.stride + 1)\n",
    "        col_weights = self.W['val'].reshape([-1,self.out_channels])\n",
    "        self.col_image = []\n",
    "        conv_out = np.zeros((N,self.out_channels,H_,W_))\n",
    "        for i in range(N):\n",
    "            img_i = x[i]\n",
    "            self.col_image_i = self.im2col(img_i,self.kernel_size[0],self.stride)\n",
    "#             print(self.col_image_i.shape)\n",
    "            conv_out[i] = np.reshape(np.dot(self.col_image_i, col_weights) + self.b['val'],(H_,W_,-1)).transpose(2,0,1)\n",
    "            self.col_image.append(self.col_image_i)\n",
    "        self.col_image = np.array(self.col_image)\n",
    "        return conv_out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        (N,Cout,H_,W_) = dout.shape\n",
    "        col_dout = dout.reshape((N,Cout,-1))\n",
    "        \n",
    "        for i in range(self.input_shape[0]):\n",
    "            self.W['grad'] += np.dot(col_dout[i], self.col_image[i]).reshape(self.W['val'].shape)\n",
    "        self.b['grad'] += np.sum(col_dout,axis=(0,2)).reshape(self.b['val'].shape)\n",
    "        \n",
    "        dout_pad = np.pad(dout, ((0, 0), (0, 0), (self.kernel_size[0] - 1, self.kernel_size[0] - 1), (self.kernel_size[0] - 1, self.kernel_size[0] - 1)), 'constant', constant_values=0)\n",
    "        \n",
    "        weight_flip = self.W['val'][:,:,::-1,::-1]\n",
    "        weight_flip = np.swapaxes(weight_flip, 0, 1)\n",
    "        weight_flip_col = weight_flip.reshape(self.in_channels, -1)\n",
    "        \n",
    "        next_dout =  np.zeros((N, self.in_channels, self.input_shape[2], self.input_shape[3]))\n",
    "        for i in range(N):\n",
    "            dout_pad_image_batch_i = dout_pad[i,:]\n",
    "            dout_pad_image_batch_i_col = self.im2col(dout_pad_image_batch_i , self.kernel_size[0], self.stride)\n",
    "            next_dout[i] = np.reshape(np.dot(weight_flip_col, np.transpose(dout_pad_image_batch_i_col)), (self.in_channels, self.input_shape[2], self.input_shape[3]))\n",
    "        \n",
    "        return next_dout\n",
    "        \n",
    "    def im2col(self, image, ksize, stride):\n",
    "        # image is a 3d tensor([channel, height, width])\n",
    "        image_col = []\n",
    "        for i in range(0, image.shape[1] - ksize + 1, stride):\n",
    "            for j in range(0, image.shape[2] - ksize + 1, stride):\n",
    "                col = image[:,i:i + ksize, j:j + ksize].reshape([-1])\n",
    "                image_col.append(col)\n",
    "        image_col = np.array(image_col)\n",
    "        return image_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2d:\n",
    "    def __init__(self, kernel_size=2, stride=2):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.mask = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        (N,Cin,H,W) = x.shape\n",
    "        H_ = int(H/self.kernel_size)\n",
    "        W_ = int(W/self.kernel_size)\n",
    "        out = np.zeros((N, Cin, H_, W_))\n",
    "        self.mask = np.zeros(x.shape)\n",
    "        for n in range(N):\n",
    "            for cin in range(Cin):\n",
    "                for h in range(0, H, self.stride):\n",
    "                    for w in range(0, W, self.stride):\n",
    "                        out[n,cin,h//self.stride,w//self.stride] = np.max(x[n,cin,h:h + self.kernel_size, w:w + self.kernel_size])\n",
    "                        i,j = np.unravel_index(np.argmax(x[n,cin,h:h + self.kernel_size, w:w + self.kernel_size]),(self.kernel_size,self.kernel_size))\n",
    "                        self.mask[n,cin,i+h,j+w] = 1\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        return np.repeat(np.repeat(dout, self.stride, axis=2), self.stride, axis=3) * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "        self.loss = 0\n",
    "        \n",
    "    def forward(self, prediction):\n",
    "        self.prediction = prediction\n",
    "        exp_prediction = np.zeros(prediction.shape)\n",
    "        self.softmax = np.zeros(prediction.shape)\n",
    "        for n in range(prediction.shape[0]):\n",
    "            prediction[n, :] -= np.max(prediction[n, :]) # 防止上溢出\n",
    "            exp_prediction[n] = np.exp(prediction[n])\n",
    "            self.softmax[n] = exp_prediction[n]/np.sum(exp_prediction[n])\n",
    "    \n",
    "        return self.softmax\n",
    "    \n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLLoss():\n",
    "    def __init__(self):\n",
    "        self.loss = 0\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        loss = 0.0\n",
    "        N = pred.shape[0]\n",
    "        for n in range(N):\n",
    "            label = target[n]\n",
    "            if pred[n,label] == 0:\n",
    "                loss += 500\n",
    "            else:\n",
    "                self.loss += -np.log(pred[n,label])\n",
    "        return self.loss / N\n",
    "        \n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get(self, Y_pred, Y_true):\n",
    "        N = Y_pred.shape[0]\n",
    "        softmax = Softmax()\n",
    "        nllLoss = NLLLoss()\n",
    "        prob = softmax.forward(Y_pred)\n",
    "        loss = nllLoss.forward(prob, Y_true)\n",
    "        dout = prob.copy()\n",
    "        dout[np.arange(N), Y_true] -= 1\n",
    "        return loss, dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    \"\"\"\n",
    "    ReLU activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #print(\"Build ReLU\")\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        #print(\"ReLU: _forward\")\n",
    "        out = np.maximum(0, X)\n",
    "        self.cache = X\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        #print(\"ReLU: _backward\")\n",
    "        X = self.cache\n",
    "        dX = np.array(dout, copy=True)\n",
    "        dX[X <= 0] = 0\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout():\n",
    "    \"\"\"\n",
    "    Dropout layer\n",
    "    \"\"\"\n",
    "    def __init__(self, p=1):\n",
    "        self.cache = None\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, X):\n",
    "        M = (np.random.rand(*X.shape) < self.p) / self.p\n",
    "        self.cache = X, M\n",
    "        return X*M\n",
    "\n",
    "    def backward(self, dout):\n",
    "        X, M = self.cache\n",
    "        dX = dout*M/self.p\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    def __init__(self, params, lr=0.001, reg=0):\n",
    "        self.parameters = params\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.parameters:\n",
    "            param['val'] -= (self.lr*param['grad'] + self.reg*param['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum():\n",
    "    def __init__(self, params, lr=0.001, momentum=0.99, reg=0):\n",
    "        self.l = len(params)\n",
    "        self.parameters = params\n",
    "        self.velocities = []\n",
    "        for param in self.parameters:\n",
    "            self.velocities.append(np.zeros(param['val'].shape))\n",
    "        self.lr = lr\n",
    "        self.rho = momentum\n",
    "        self.reg = reg\n",
    "\n",
    "    def step(self):\n",
    "        for i in range(self.l):\n",
    "            self.velocities[i] = self.rho*self.velocities[i] + (1-self.rho)*self.parameters[i]['grad']\n",
    "            self.parameters[i]['val'] -= (self.lr*self.velocities[i] + self.reg*self.parameters[i]['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5():\n",
    "    def __init__(self):\n",
    "        self.conv1 = Conv2d(1, 6, 5)\n",
    "        self.ReLU1 = ReLU()\n",
    "        self.pool1 = MaxPool2d(2,2)\n",
    "        self.conv2 = Conv2d(6, 16, 5)\n",
    "        self.ReLU2 = ReLU()\n",
    "        self.pool2 = MaxPool2d(2,2)\n",
    "        self.FC1 = FC(16*4*4, 120)\n",
    "        self.ReLU3 = ReLU()\n",
    "        self.FC2 = FC(120, 84)\n",
    "        self.ReLU4 = ReLU()\n",
    "        self.FC3 = FC(84, 10)\n",
    "\n",
    "        self.p2_shape = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        h1 = self.conv1.forward(X)\n",
    "        a1 = self.ReLU1.forward(h1)\n",
    "        p1 = self.pool1.forward(a1)\n",
    "        h2 = self.conv2.forward(p1)\n",
    "        a2 = self.ReLU2.forward(h2)\n",
    "        p2 = self.pool2.forward(a2)\n",
    "        self.p2_shape = p2.shape\n",
    "        fl = p2.reshape(X.shape[0],-1) # Flatten\n",
    "        h3 = self.FC1.forward(fl)\n",
    "        a3 = self.ReLU3.forward(h3)\n",
    "        h4 = self.FC2.forward(a3)\n",
    "        a5 = self.ReLU4.forward(h4)\n",
    "        h5 = self.FC3.forward(a5)\n",
    "        return h5\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = self.FC3.backward(dout)\n",
    "        dout = self.ReLU4.backward(dout)\n",
    "        dout = self.FC2.backward(dout)\n",
    "        dout = self.ReLU3.backward(dout)\n",
    "        dout = self.FC1.backward(dout)\n",
    "        dout = dout.reshape(self.p2_shape) # reshape\n",
    "        dout = self.pool2.backward(dout)\n",
    "        dout = self.ReLU2.backward(dout)\n",
    "        dout = self.conv2.backward(dout)\n",
    "        dout = self.pool1.backward(dout)\n",
    "        dout = self.ReLU1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "        \n",
    "    def get_params(self):\n",
    "        return [self.conv1.W, self.conv1.b, self.conv2.W, self.conv2.b, self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b]\n",
    "\n",
    "    def set_params(self, params):\n",
    "        [self.conv1.W, self.conv1.b, self.conv2.W, self.conv2.b, self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, Y, batch_size):\n",
    "    N = len(X)\n",
    "    i = random.randint(1, N-batch_size)\n",
    "    return X[i:i+batch_size], Y[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = load()\n",
    "X_train, X_test = X_train/float(255), X_test/float(255)\n",
    "X_train -= np.mean(X_train)\n",
    "X_test -= np.mean(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5()\n",
    "losses = []\n",
    "optim = SGDMomentum(model.get_params(), lr=0.0001, momentum=0.80, reg=0.00003)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% iter: 0, loss: 41.4633023728112\n",
      "0.4% iter: 100, loss: 2.3412491573794174\n",
      "0.8% iter: 200, loss: 2.3035299930947026\n",
      "1.2% iter: 300, loss: 2.5868564395210836\n",
      "1.6% iter: 400, loss: 2.3145510668052163\n",
      "2.0% iter: 500, loss: 2.3653973036309646\n",
      "2.4% iter: 600, loss: 2.326368302701193\n",
      "2.8% iter: 700, loss: 2.3943870316200835\n",
      "3.2% iter: 800, loss: 2.3214183206224988\n",
      "3.6% iter: 900, loss: 2.6561853775115285\n",
      "4.0% iter: 1000, loss: 2.300080458085136\n",
      "4.4% iter: 1100, loss: 2.3646496458984996\n",
      "4.8% iter: 1200, loss: 2.436741560898203\n",
      "5.2% iter: 1300, loss: 2.2166945777290255\n",
      "5.6% iter: 1400, loss: 2.2846001574782635\n",
      "6.0% iter: 1500, loss: 2.4481147328962214\n",
      "6.4% iter: 1600, loss: 2.282667533696766\n",
      "6.8% iter: 1700, loss: 2.4470688146979103\n",
      "7.2% iter: 1800, loss: 2.429213668543574\n",
      "7.6% iter: 1900, loss: 2.319323173972759\n",
      "8.0% iter: 2000, loss: 2.4317727654849333\n",
      "8.4% iter: 2100, loss: 2.3924391044026754\n",
      "8.8% iter: 2200, loss: 2.314277452229027\n",
      "9.2% iter: 2300, loss: 2.3479829450615597\n",
      "9.6% iter: 2400, loss: 2.2109813572867925\n",
      "10.0% iter: 2500, loss: 2.262534416089073\n",
      "10.4% iter: 2600, loss: 2.4099256696737505\n",
      "10.8% iter: 2700, loss: 2.434117002146645\n",
      "11.2% iter: 2800, loss: 2.3043714287075954\n",
      "11.6% iter: 2900, loss: 2.3850385108534673\n",
      "12.0% iter: 3000, loss: 2.496964898796496\n",
      "12.4% iter: 3100, loss: 2.2918657865669396\n",
      "12.8% iter: 3200, loss: 2.2210270585640552\n",
      "13.2% iter: 3300, loss: 2.3387942384621234\n",
      "13.6% iter: 3400, loss: 2.4554625504570637\n",
      "14.0% iter: 3500, loss: 2.543407710301977\n",
      "14.4% iter: 3600, loss: 2.3160763584910704\n",
      "14.8% iter: 3700, loss: 2.431595407193247\n",
      "15.2% iter: 3800, loss: 2.4852646403148815\n",
      "15.6% iter: 3900, loss: 2.382448858656817\n",
      "16.0% iter: 4000, loss: 2.4910616878809853\n",
      "16.4% iter: 4100, loss: 2.2701995056145936\n",
      "16.8% iter: 4200, loss: 2.5652404615685165\n",
      "17.2% iter: 4300, loss: 2.3091342470376808\n",
      "17.6% iter: 4400, loss: 2.1929268108226183\n",
      "18.0% iter: 4500, loss: 2.3380510021553884\n",
      "18.4% iter: 4600, loss: 2.4049925243816395\n",
      "18.8% iter: 4700, loss: 2.6603237533086417\n",
      "19.2% iter: 4800, loss: 2.537211187573285\n",
      "19.6% iter: 4900, loss: 2.325453039984109\n",
      "20.0% iter: 5000, loss: 2.4990172777066655\n",
      "20.4% iter: 5100, loss: 2.23054864822411\n",
      "20.8% iter: 5200, loss: 2.3649897004699003\n",
      "21.2% iter: 5300, loss: 2.3223321988155075\n",
      "21.6% iter: 5400, loss: 2.3773967802988274\n",
      "22.0% iter: 5500, loss: 2.6598009141532124\n",
      "22.4% iter: 5600, loss: 2.25432298154784\n",
      "22.8% iter: 5700, loss: 2.3766403969361485\n",
      "23.2% iter: 5800, loss: 2.449973144255447\n",
      "23.6% iter: 5900, loss: 2.4608908323737855\n",
      "24.0% iter: 6000, loss: 2.3602733177670396\n",
      "24.4% iter: 6100, loss: 2.264446714359937\n",
      "24.8% iter: 6200, loss: 2.4000824254090554\n",
      "25.2% iter: 6300, loss: 2.424145677685592\n",
      "25.6% iter: 6400, loss: 2.399730765092281\n",
      "26.0% iter: 6500, loss: 2.3471842355112456\n",
      "26.4% iter: 6600, loss: 2.2594463010328756\n",
      "26.8% iter: 6700, loss: 2.1953590233883657\n",
      "27.2% iter: 6800, loss: 2.2593273659232653\n",
      "27.6% iter: 6900, loss: 2.2816526468983613\n",
      "28.0% iter: 7000, loss: 2.2350580940345433\n",
      "28.4% iter: 7100, loss: 2.3171058683128583\n",
      "28.8% iter: 7200, loss: 2.548839021221546\n",
      "29.2% iter: 7300, loss: 2.3133222213050617\n",
      "29.6% iter: 7400, loss: 2.2120465814157964\n",
      "30.0% iter: 7500, loss: 2.3425115640503082\n",
      "30.4% iter: 7600, loss: 2.2514912966027048\n",
      "30.8% iter: 7700, loss: 2.4107243257227204\n",
      "31.2% iter: 7800, loss: 2.3991772765106623\n",
      "31.6% iter: 7900, loss: 2.383797726240751\n",
      "32.0% iter: 8000, loss: 2.374543918319014\n",
      "32.4% iter: 8100, loss: 2.4102946743748976\n",
      "32.8% iter: 8200, loss: 2.3336535152451625\n",
      "33.2% iter: 8300, loss: 2.3546728258910687\n",
      "33.6% iter: 8400, loss: 2.366287767109888\n",
      "34.0% iter: 8500, loss: 2.3029976907056904\n",
      "34.4% iter: 8600, loss: 2.3243577898232823\n",
      "34.8% iter: 8700, loss: 2.329120179194054\n",
      "35.2% iter: 8800, loss: 2.323125243715175\n",
      "35.6% iter: 8900, loss: 2.3547615697854467\n",
      "36.0% iter: 9000, loss: 2.331482061252671\n",
      "36.4% iter: 9100, loss: 2.363476755081602\n",
      "36.8% iter: 9200, loss: 2.3207705032335566\n",
      "37.2% iter: 9300, loss: 2.3525616470759285\n",
      "37.6% iter: 9400, loss: 2.414465273426842\n",
      "38.0% iter: 9500, loss: 2.3705016923011466\n",
      "38.4% iter: 9600, loss: 2.3233387522070093\n",
      "38.8% iter: 9700, loss: 2.3139621479422754\n",
      "39.2% iter: 9800, loss: 2.362468780920903\n",
      "39.6% iter: 9900, loss: 2.443054946760426\n",
      "40.0% iter: 10000, loss: 2.342067988495489\n",
      "40.4% iter: 10100, loss: 2.382337414573726\n",
      "40.8% iter: 10200, loss: 2.360311273134394\n",
      "41.2% iter: 10300, loss: 2.3160575080406103\n",
      "41.6% iter: 10400, loss: 2.4285872057340234\n",
      "42.0% iter: 10500, loss: 2.25715192161723\n",
      "42.4% iter: 10600, loss: 2.341747147237587\n",
      "42.8% iter: 10700, loss: 2.365273527766581\n",
      "43.2% iter: 10800, loss: 2.3286565493179334\n",
      "43.6% iter: 10900, loss: 2.3582388776629073\n",
      "44.0% iter: 11000, loss: 2.2919117914708758\n",
      "44.4% iter: 11100, loss: 2.309640865741912\n",
      "44.8% iter: 11200, loss: 2.323167291259598\n",
      "45.2% iter: 11300, loss: 2.342587508323168\n",
      "45.6% iter: 11400, loss: 2.3378509756904085\n",
      "46.0% iter: 11500, loss: 2.4600605309245265\n",
      "46.4% iter: 11600, loss: 2.3658427264174464\n",
      "46.8% iter: 11700, loss: 2.3077763493815677\n",
      "47.2% iter: 11800, loss: 2.343765968827425\n",
      "47.6% iter: 11900, loss: 2.406268823007646\n",
      "48.0% iter: 12000, loss: 2.2926271927621884\n",
      "48.4% iter: 12100, loss: 2.2855701038437948\n",
      "48.8% iter: 12200, loss: 2.3287117014304237\n",
      "49.2% iter: 12300, loss: 2.359212107486246\n",
      "49.6% iter: 12400, loss: 2.314458628000414\n",
      "50.0% iter: 12500, loss: 2.35280254686052\n",
      "50.4% iter: 12600, loss: 2.317991020904614\n",
      "50.8% iter: 12700, loss: 2.3981548027493615\n",
      "51.2% iter: 12800, loss: 2.2783557825725804\n",
      "51.6% iter: 12900, loss: 2.2735663652910727\n",
      "52.0% iter: 13000, loss: 2.304461320206592\n",
      "52.4% iter: 13100, loss: 2.4105390508288993\n",
      "52.8% iter: 13200, loss: 2.293485947826389\n",
      "53.2% iter: 13300, loss: 2.2878892032041707\n",
      "53.6% iter: 13400, loss: 2.261442731865491\n",
      "54.0% iter: 13500, loss: 2.3679234587205302\n",
      "54.4% iter: 13600, loss: 2.3113864762927614\n",
      "54.8% iter: 13700, loss: 2.3391277616920916\n",
      "55.2% iter: 13800, loss: 2.2918616897787265\n",
      "55.6% iter: 13900, loss: 2.308932684306367\n",
      "56.0% iter: 14000, loss: 2.3816286520082195\n",
      "56.4% iter: 14100, loss: 2.333001185334763\n",
      "56.8% iter: 14200, loss: 2.343955942545631\n",
      "57.2% iter: 14300, loss: 2.314841589061046\n",
      "57.6% iter: 14400, loss: 2.2770031307719942\n",
      "58.0% iter: 14500, loss: 2.331991454880785\n",
      "58.4% iter: 14600, loss: 2.2908124015939118\n",
      "58.8% iter: 14700, loss: 2.3160622001820315\n",
      "59.2% iter: 14800, loss: 2.2617798166504866\n",
      "59.6% iter: 14900, loss: 2.308723991617272\n",
      "60.0% iter: 15000, loss: 2.354237120505718\n",
      "60.4% iter: 15100, loss: 2.2552522621591726\n",
      "60.8% iter: 15200, loss: 2.305222586248822\n",
      "61.2% iter: 15300, loss: 2.2682802678018894\n",
      "61.6% iter: 15400, loss: 2.3236991712853006\n",
      "62.0% iter: 15500, loss: 2.318826650837192\n",
      "62.4% iter: 15600, loss: 2.3373575096255443\n",
      "62.8% iter: 15700, loss: 2.313693279122455\n",
      "63.2% iter: 15800, loss: 2.290883804112933\n",
      "63.6% iter: 15900, loss: 2.3513728787565356\n",
      "64.0% iter: 16000, loss: 2.278846574547878\n",
      "64.4% iter: 16100, loss: 2.3467773921754085\n",
      "64.8% iter: 16200, loss: 2.2977311587414206\n",
      "65.2% iter: 16300, loss: 2.290739747254306\n",
      "65.6% iter: 16400, loss: 2.2938718248631296\n",
      "66.0% iter: 16500, loss: 2.324044729752085\n",
      "66.4% iter: 16600, loss: 2.3174987605538573\n",
      "66.8% iter: 16700, loss: 2.2857639278522357\n",
      "67.2% iter: 16800, loss: 2.3330451805432064\n",
      "67.6% iter: 16900, loss: 2.3030102153191576\n",
      "68.0% iter: 17000, loss: 2.451248174186133\n",
      "68.4% iter: 17100, loss: 2.291013709939649\n",
      "68.8% iter: 17200, loss: 2.3134530332326375\n",
      "69.2% iter: 17300, loss: 2.2749534717232325\n",
      "69.6% iter: 17400, loss: 2.2914733247200734\n",
      "70.0% iter: 17500, loss: 2.3902012907302606\n",
      "70.4% iter: 17600, loss: 2.342001475361532\n",
      "70.8% iter: 17700, loss: 2.278511144400236\n",
      "71.2% iter: 17800, loss: 2.3553455449846714\n",
      "71.6% iter: 17900, loss: 2.2864626553062495\n",
      "72.0% iter: 18000, loss: 2.29683737732445\n",
      "72.4% iter: 18100, loss: 2.2930745315879144\n",
      "72.8% iter: 18200, loss: 2.3120327922494206\n",
      "73.2% iter: 18300, loss: 2.409732394495372\n",
      "73.6% iter: 18400, loss: 2.3022472109004366\n",
      "74.0% iter: 18500, loss: 2.305771079428884\n",
      "74.4% iter: 18600, loss: 2.3704800979024863\n",
      "74.8% iter: 18700, loss: 2.305411863466423\n",
      "75.2% iter: 18800, loss: 2.261129553978054\n",
      "75.6% iter: 18900, loss: 2.3202734604170114\n",
      "76.0% iter: 19000, loss: 2.2920294896904236\n",
      "76.4% iter: 19100, loss: 2.3169202036649894\n",
      "76.8% iter: 19200, loss: 2.3208945674359427\n",
      "77.2% iter: 19300, loss: 2.280331289534082\n",
      "77.6% iter: 19400, loss: 2.3792825447158514\n",
      "78.0% iter: 19500, loss: 2.30380667614613\n",
      "78.4% iter: 19600, loss: 2.322652343327055\n",
      "78.8% iter: 19700, loss: 2.2491376341791365\n",
      "79.2% iter: 19800, loss: 2.3054692696815624\n",
      "79.6% iter: 19900, loss: 2.3446150582392677\n",
      "80.0% iter: 20000, loss: 2.3259022655857393\n",
      "80.4% iter: 20100, loss: 2.274049462114758\n",
      "80.8% iter: 20200, loss: 2.38019903111712\n",
      "81.2% iter: 20300, loss: 2.30424622146711\n",
      "81.6% iter: 20400, loss: 2.277760816559187\n",
      "82.0% iter: 20500, loss: 2.3434776135087545\n",
      "82.4% iter: 20600, loss: 2.320461398138985\n",
      "82.8% iter: 20700, loss: 2.3337483083941457\n",
      "83.2% iter: 20800, loss: 2.333725574808795\n",
      "83.6% iter: 20900, loss: 2.326462546047452\n",
      "84.0% iter: 21000, loss: 2.2678180635709655\n",
      "84.4% iter: 21100, loss: 2.3519650612477245\n",
      "84.8% iter: 21200, loss: 2.291839990843096\n",
      "85.2% iter: 21300, loss: 2.314301910553633\n",
      "85.6% iter: 21400, loss: 2.3067238671963763\n",
      "86.0% iter: 21500, loss: 2.2981229211834275\n",
      "86.4% iter: 21600, loss: 2.243861275971755\n",
      "86.8% iter: 21700, loss: 2.3313898495539185\n",
      "87.2% iter: 21800, loss: 2.2932967207122203\n",
      "87.6% iter: 21900, loss: 2.3532529079739044\n",
      "88.0% iter: 22000, loss: 2.305299098988171\n",
      "88.4% iter: 22100, loss: 2.284237711446021\n",
      "88.8% iter: 22200, loss: 2.3437354397546457\n",
      "89.2% iter: 22300, loss: 2.3248167273211426\n",
      "89.6% iter: 22400, loss: 2.2837981767097455\n",
      "90.0% iter: 22500, loss: 2.2969981789221974\n",
      "90.4% iter: 22600, loss: 2.3223972927260426\n",
      "90.8% iter: 22700, loss: 2.331885332527978\n",
      "91.2% iter: 22800, loss: 2.308096706628901\n",
      "91.6% iter: 22900, loss: 2.3244096945371475\n",
      "92.0% iter: 23000, loss: 2.333727400449329\n",
      "92.4% iter: 23100, loss: 2.329427509843749\n",
      "92.8% iter: 23200, loss: 2.3388589612241435\n",
      "93.2% iter: 23300, loss: 2.287342739464495\n",
      "93.6% iter: 23400, loss: 2.329550421916803\n",
      "94.0% iter: 23500, loss: 2.2810352182782996\n",
      "94.4% iter: 23600, loss: 2.3019851386540933\n",
      "94.8% iter: 23700, loss: 2.330812317035376\n",
      "95.2% iter: 23800, loss: 2.279208029335459\n",
      "95.6% iter: 23900, loss: 2.325733625899654\n",
      "96.0% iter: 24000, loss: 2.266247855642264\n",
      "96.4% iter: 24100, loss: 2.2731903310262886\n",
      "96.8% iter: 24200, loss: 2.317568636343834\n",
      "97.2% iter: 24300, loss: 2.3079242949800625\n",
      "97.6% iter: 24400, loss: 2.2791658951472153\n",
      "98.0% iter: 24500, loss: 2.3463698807092146\n",
      "98.4% iter: 24600, loss: 2.2904627034623912\n",
      "98.8% iter: 24700, loss: 2.303794407219956\n",
      "99.2% iter: 24800, loss: 2.318586045130873\n",
      "99.6% iter: 24900, loss: 2.2671780849214245\n"
     ]
    }
   ],
   "source": [
    "ITER = 25000\n",
    "for i in range(ITER):\n",
    "    # get batch, make onehot\n",
    "    X_batch, Y_batch = get_batch(X_train, Y_train, batch_size)\n",
    "\n",
    "    # forward, loss, backward, step\n",
    "    X_batch = X_batch.reshape((batch_size, 1, 28, 28))\n",
    "    Y_pred = model.forward(X_batch)\n",
    "#     print(Y_pred)\n",
    "    loss, dout = criterion.get(Y_pred, Y_batch)\n",
    "#     print(dout)\n",
    "    model.backward(dout)\n",
    "    optim.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"%s%% iter: %s, loss: %s\" % (100*i/ITER,i, loss))\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = X_train[0:2].reshape(2,1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 25)\n",
      "(576, 25)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 6, 24, 24)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = Conv2d(1,6,5)\n",
    "out = conv2d.forward(batch)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = out.copy() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dout = conv2d.backward(real-out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ -5.60940891, -10.83559184, -10.82332433, ...,  -0.57075499,\n",
       "           -0.5830225 ,   0.97290732],\n",
       "         [ -8.84154755, -18.74743301, -20.49830635, ...,  -1.93744294,\n",
       "           -0.1865696 ,   0.78923514],\n",
       "         [-12.66774302, -20.3079634 , -23.26653143, ...,   0.06372398,\n",
       "            3.02229201,   0.48989108],\n",
       "         ...,\n",
       "         [ -0.13819793,   2.60949862,   1.72871981, ...,   5.86016799,\n",
       "            6.7409468 ,  -1.06757788],\n",
       "         [  3.68799755,   4.17002901,   4.49694489, ...,   3.85900107,\n",
       "            3.5320852 ,  -0.76823382],\n",
       "         [ -0.19715702,   0.70554064,   1.61050725, ...,   1.68101308,\n",
       "            0.77604646,  -0.6356793 ]]],\n",
       "\n",
       "\n",
       "       [[[ -5.60940891, -10.83559184, -10.82332433, ...,  -0.57075499,\n",
       "           -0.5830225 ,   0.97290732],\n",
       "         [ -8.84154755, -18.74743301, -20.49830635, ...,  -1.93744294,\n",
       "           -0.1865696 ,   0.78923514],\n",
       "         [-12.66774302, -20.3079634 , -23.26653143, ...,   0.06372398,\n",
       "            3.02229201,   0.48989108],\n",
       "         ...,\n",
       "         [ -0.13819793,   2.60949862,   1.72871981, ...,   5.86016799,\n",
       "            6.7409468 ,  -1.06757788],\n",
       "         [  3.68799755,   4.17002901,   4.49694489, ...,   3.85900107,\n",
       "            3.5320852 ,  -0.76823382],\n",
       "         [ -0.19715702,   0.70554064,   1.61050725, ...,   1.68101308,\n",
       "            0.77604646,  -0.6356793 ]]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b4068122c8>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pool2d = MaxPool2d()\n",
    "out = pool2d.forward(batch)\n",
    "plt.imshow(pool2d.mask[0].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b402101d08>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(out[0,0,:,:].reshape(24,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine Learning",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
