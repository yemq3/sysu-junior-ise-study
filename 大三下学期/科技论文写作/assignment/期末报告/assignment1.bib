@article{Ba2016,
abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
archivePrefix = {arXiv},
arxivId = {1607.06450},
author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
eprint = {1607.06450},
file = {:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ba, Kiros, Hinton - 2016 - Layer Normalization.pdf:pdf},
month = {jul},
title = {{Layer Normalization}},
url = {http://arxiv.org/abs/1607.06450},
year = {2016}
}
@article{Li2020,
abstract = {Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.},
archivePrefix = {arXiv},
arxivId = {1908.07873},
author = {Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
doi = {10.1109/MSP.2020.2975749},
eprint = {1908.07873},
issn = {15580792},
journal = {IEEE Signal Processing Magazine},
month = {may},
number = {3},
pages = {50--60},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Federated Learning: Challenges, Methods, and Future Directions}},
volume = {37},
year = {2020}
}
@article{McMahan2016,
abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
archivePrefix = {arXiv},
arxivId = {1602.05629},
author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Arcas, Blaise Ag{\"{u}}era y},
eprint = {1602.05629},
journal = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017},
month = {feb},
publisher = {PMLR},
title = {{Communication-Efficient Learning of Deep Networks from Decentralized Data}},
url = {http://arxiv.org/abs/1602.05629},
year = {2016}
}
@article{International,
author = {of the International, N Strom - Sixteenth Annual Conference and undefined 2015},
journal = {isca-speech.org},
title = {{Scalable distributed DNN training using commodity GPU cloud computing}},
url = {https://www.isca-speech.org/archive/interspeech{\_}2015/i15{\_}1488.html}
}
@inproceedings{Garg2009,
abstract = {We present an algorithm for finding an s-sparse vector x that minimizes the square error ||y - $\Phi$x||2 where $\Phi$ satisfies the restricted isometry property (RIP), with isometric constant $\Delta$2s {\textless} 1/3. Our algorithm, called GraDeS (Gradient Descent with Sparsification) iteratively updates x as: x ← Hs (x + 1/gamma;. $\Phi$T(y-$\Phi$x) where $\gamma$ {\textgreater} 1 and Hs sets all but s largest magnitude coordinates to zero. GraDeS converges to the correct solution in constant number of iterations. The condition $\Delta$2s {\textless} 1/3 is most general for which a near-linear time algorithm is known. In comparison, the best condition under which a polynomialtime algorithm is known, is $\Delta$2s {\textless} 2-1. Our Matlab implementation of GraDeS outperforms previously proposed algorithms like Subspace Pursuit, StOMP, OMP, and Lasso by an order of magnitude. Curiously, our experiments also uncovered cases where L1-regularized regression (Lasso) fails but GraDeS finds the correct solution. Copyright 2009.},
author = {Garg, Rahul and Khandekar, Rohit},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/1553374.1553417},
file = {:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garg, Khandekar - 2009 - Gradient descent with sparsification An iterative algorithm for sparse recovery with restricted isometry pro(2).pdf:pdf},
isbn = {9781605585161},
title = {{Gradient descent with sparsification : An iterative algorithm for sparse recovery with restricted isometry property}},
volume = {382},
year = {2009}
}
@article{Wu2018,
abstract = {Large-scale distributed optimization is of great importance in various applications. For data-parallel based distributed learning, the inter-node gradient communication often becomes the performance bottleneck. In this paper, we propose the error compensated quantized stochastic gradient descent algorithm to improve the training efficiency. Local gradients are quantized to reduce the communication overhead, and accumulated quantization error is utilized to speed up the convergence. Furthermore, we present theoretical analysis on the convergence behaviour, and demonstrate its advantage over competitors. Extensive experiments indicate that our algorithm can compress gradients by a factor of up to two magnitudes without performance degradation.},
archivePrefix = {arXiv},
arxivId = {1806.08054},
author = {Wu, Jiaxiang and Huang, Weidong and Huang, Junzhou and Zhang, Tong},
eprint = {1806.08054},
file = {:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2018 - Error Compensated Quantized SGD and its Applications to Large-scale Distributed Optimization.pdf:pdf},
journal = {35th International Conference on Machine Learning, ICML 2018},
month = {jun},
pages = {8472--8483},
publisher = {International Machine Learning Society (IMLS)},
title = {{Error Compensated Quantized SGD and its Applications to Large-scale Distributed Optimization}},
url = {http://arxiv.org/abs/1806.08054},
volume = {12},
year = {2018}
}
@article{Kairouz2019,
abstract = {Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.},
archivePrefix = {arXiv},
arxivId = {1912.04977},
author = {Kairouz, Peter and McMahan, H. Brendan and Avent, Brendan and Bellet, Aur{\'{e}}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Keith and Charles, Zachary and Cormode, Graham and Cummings, Rachel and D'Oliveira, Rafael G. L. and Rouayheb, Salim El and Evans, David and Gardner, Josh and Garrett, Zachary and Gasc{\'{o}}n, Adri{\`{a}} and Ghazi, Badih and Gibbons, Phillip B. and Gruteser, Marco and Harchaoui, Zaid and He, Chaoyang and He, Lie and Huo, Zhouyuan and Hutchinson, Ben and Hsu, Justin and Jaggi, Martin and Javidi, Tara and Joshi, Gauri and Khodak, Mikhail and Kone{\v{c}}n{\'{y}}, Jakub and Korolova, Aleksandra and Koushanfar, Farinaz and Koyejo, Sanmi and Lepoint, Tancr{\`{e}}de and Liu, Yang and Mittal, Prateek and Mohri, Mehryar and Nock, Richard and {\"{O}}zg{\"{u}}r, Ayfer and Pagh, Rasmus and Raykova, Mariana and Qi, Hang and Ramage, Daniel and Raskar, Ramesh and Song, Dawn and Song, Weikang and Stich, Sebastian U. and Sun, Ziteng and Suresh, Ananda Theertha and Tram{\`{e}}r, Florian and Vepakomma, Praneeth and Wang, Jianyu and Xiong, Li and Xu, Zheng and Yang, Qiang and Yu, Felix X. and Yu, Han and Zhao, Sen},
eprint = {1912.04977},
file = {:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kairouz et al. - 2019 - Advances and Open Problems in Federated Learning.pdf:pdf},
month = {dec},
title = {{Advances and Open Problems in Federated Learning}},
url = {http://arxiv.org/abs/1912.04977},
year = {2019}
}
@article{Hard2018,
abstract = {We train a recurrent neural network language model using a distributed, on-device learning framework called federated learning for the purpose of next-word prediction in a virtual keyboard for smartphones. Server-based training using stochastic gradient descent is compared with training on client devices using the Federated Averaging algorithm. The federated algorithm, which enables training on a higher-quality dataset for this use case, is shown to achieve better prediction recall. This work demonstrates the feasibility and benefit of training language models on client devices without exporting sensitive user data to servers. The federated learning environment gives users greater control over the use of their data and simplifies the task of incorporating privacy by default with distributed training and aggregation across a population of client devices.},
archivePrefix = {arXiv},
arxivId = {1811.03604},
author = {Hard, Andrew and Rao, Kanishka and Mathews, Rajiv and Ramaswamy, Swaroop and Beaufays, Fran{\c{c}}oise and Augenstein, Sean and Eichner, Hubert and Kiddon, Chlo{\'{e}} and Ramage, Daniel},
eprint = {1811.03604},
file = {:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hard et al. - 2018 - Federated Learning for Mobile Keyboard Prediction.pdf:pdf},
month = {nov},
title = {{Federated Learning for Mobile Keyboard Prediction}},
url = {http://arxiv.org/abs/1811.03604},
year = {2018}
}
@inproceedings{Aji2017,
abstract = {We make distributed stochastic gradient descent faster by exchanging sparse updates instead of dense updates. Gradient updates are positively skewed as most updates are near zero, so we map the 99{\%} smallest updates (by absolute value) to zero then exchange sparse matrices. This method can be combined with quantization to further improve the compression. We explore different configurations and apply them to neural machine translation and MNIST image classification tasks. Most configurations work on MNIST, whereas different configurations reduce convergence rate on the more complex translation task. Our experiments show that we can achieve up to 49{\%} speed up on MNIST and 22{\%} on NMT without damaging the final accuracy or BLEU.},
archivePrefix = {arXiv},
arxivId = {1704.05021},
author = {Aji, Alham Fikri and Heafield, Kenneth},
booktitle = {EMNLP 2017 - Conference on Empirical Methods in Natural Language Processing, Proceedings},
doi = {10.18653/v1/d17-1045},
eprint = {1704.05021},
file = {:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aji, Heafield - 2017 - Sparse communication for distributed gradient descent.pdf:pdf;:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aji, Heafield - 2017 - Sparse communication for distributed gradient descent(2).pdf:pdf},
isbn = {9781945626838},
pages = {440--445},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Sparse communication for distributed gradient descent}},
year = {2017}
}
@article{Lin2017,
abstract = {Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9{\%} of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.},
archivePrefix = {arXiv},
arxivId = {1712.01887},
author = {Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J.},
eprint = {1712.01887},
file = {:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - 2017 - Deep Gradient Compression Reducing the Communication Bandwidth for Distributed Training(2).pdf:pdf},
month = {dec},
title = {{Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training}},
url = {http://arxiv.org/abs/1712.01887},
year = {2017}
}
@article{Chen,
abstract = {Highly distributed training of Deep Neural Networks (DNNs) on future compute platforms (offering 100 of TeraOps/s of computational capacity) is expected to be severely communication constrained. To overcome this limitation, new gradient compression techniques are needed that are computationally friendly, applicable to a wide variety of layers seen in Deep Neural Networks and adaptable to variations in network architectures as well as their hyper-parameters. In this paper we introduce a novel technique - the Adaptive Residual Gradient Compression (AdaComp) scheme. AdaComp is based on localized selection of gradient residues and automatically tunes the compression rate depending on local activity. We show excellent results on a wide spectrum of state of the art Deep Learning models in multiple domains (vision, speech, language), datasets (MNIST, CIFAR10, ImageNet, BN50, Shakespeare), optimizers (SGD with momentum, Adam) and network parameters (number of learners, minibatch-size etc.). Exploiting both sparsity and quantization, we demonstrate end-to-end compression rates of ∼200× for fully-connected and recurrent layers, and ∼40× for convolutional layers, without any noticeable degradation in model accuracies.},
author = {Chen, Chia Yu and Choi, Jungwook and Brand, Daniel and Agrawal, Ankur and Zhang, Wei and Gopalakrishnan, Kailash},
file = {:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2018 - ADaComP Adaptive residual gradient compression for data-parallel distributed training.pdf:pdf},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
pages = {2827--2835},
title = {{ADaComP: Adaptive residual gradient compression for data-parallel distributed training}},
url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16859},
year = {2018}
}
@article{Dryden2016,
abstract = {We study data-parallel training of deep neural networks on high-performance computing infrastructure. The key problem with scaling data-parallel training is avoiding severe communication/computation imbalance. We explore quantizing gradient updates before communication to reduce bandwidth requirements and compare it against a baseline implementation that uses the MPI allreduce routine. We port two existing quantization approaches, one-bit and threshold, and develop our own adaptive quantization algorithm. The performance of these algorithms is evaluated and compared with MPI-Allreduce when training models for the MNIST dataset and on a synthetic benchmark. On an HPC system, MPI-Allreduce outperforms the existing quantization approaches. Our adaptive quantization is comparable or superior for large layers without sacrificing accuracy. It is 1.76 times faster than the next best approach for the largest layers in our benchmark and achieves near-linear speedup in data-parallel training.},
author = {Dryden, Nikoli and Moon, Tim and Jacobs, Sam Ade and {Van Essen}, Brian},
doi = {10.1109/MLHPC.2016.4},
file = {:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dryden et al. - 2017 - Communication quantization for data-parallel training of deep neural networks(2).pdf:pdf},
isbn = {9781509038824},
journal = {Proceedings of MLHPC 2016: Machine Learning in HPC Environments - Held in conjunction with SC 2016: The International Conference for High Performance Computing, Networking, Storage and Analysis},
pages = {1--8},
title = {{Communication quantization for data-parallel training of deep neural networks}},
url = {https://github.com/LLNL/lbann.},
year = {2017}
}
@techreport{Zhou,
abstract = {We propose DoReFa-Net, a method to train convolutional neural networks that have low bitwidth weights and activations using low bitwidth parameter gradients. In particular, during backward pass, parameter gradients are stochastically quantized to low bitwidth numbers before being propagated to convolutional layers. As convolutions during forward/backward passes can now operate on low bitwidth weights and activations/gradients respectively, DoReFa-Net can use bit convolution kernels to accelerate both training and inference. Moreover, as bit convolutions can be efficiently implemented on CPU, FPGA, ASIC and GPU, DoReFa-Net opens the way to accelerate training of low bitwidth neural network on these hardware. Our experiments on SVHN and ImageNet datasets prove that DoReFa-Net can achieve comparable prediction accuracy as 32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has 1-bit weights, 2-bit activations, can be trained from scratch using 6-bit gradients to get 46.1$\backslash${\%} top-1 accuracy on ImageNet validation set. The DoReFa-Net AlexNet model is released publicly.},
archivePrefix = {arXiv},
arxivId = {1606.06160},
author = {Zhou, Shuchang and Wu, Yuxin and Ni, Zekun and Zhou, Xinyu and Wen, He and Zou, Yuheng},
booktitle = {arxiv.org},
eprint = {1606.06160},
file = {:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2016 - DoReFa-Net Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients.pdf:pdf},
title = {{DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients}},
url = {http://arxiv.org/abs/1606.06160},
year = {2016}
}
@techreport{Wen,
abstract = {High network communication cost for synchronizing gradients and parameters is the well-known bottleneck of distributed training. In this work, we propose TernGrad that uses ternary gradients to accelerate distributed deep learning in data parallelism. Our approach requires only three numerical levels {\{}-1, 0, 1{\}}, which can aggressively reduce the communication time. We mathematically prove the convergence of TernGrad under the assumption of a bound on gradients. Guided by the bound, we propose layer-wise ternarizing and gradient clipping to improve its convergence. Our experiments show that applying TernGrad on AlexNet doesn't incur any accuracy loss and can even improve accuracy. The accuracy loss of GoogLeNet induced by TernGrad is less than 2{\%} on average. Finally, a performance model is proposed to study the scalability of TernGrad. Experiments show significant speed gains for various deep neural networks. Our source code is available[1].},
archivePrefix = {arXiv},
arxivId = {1705.07878},
author = {Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1705.07878},
file = {:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wen et al. - 2017 - TernGrad Ternary gradients to reduce communication in distributed deep learning.pdf:pdf},
issn = {10495258},
pages = {1510--1520},
title = {{TernGrad: Ternary gradients to reduce communication in distributed deep learning}},
url = {https://github.com/wenwei202/terngrad},
volume = {2017-Decem},
year = {2017}
}
@techreport{Seide2014,
abstract = {We show empirically that in SGD training of deep neural networks, one can, at no or nearly no loss of accuracy, quantize the gradients aggressively-to but one bit per value-if the quantization error is carried forward across minibatches (error feedback). This size reduction makes it feasible to parallelize SGD through data-parallelism with fast processors like recent GPUs. We implement data-parallel deterministically distributed SGD by combining this finding with AdaGrad, automatic minibatch-size selection, double buffering, and model parallelism. Unexpectedly, quantization benefits AdaGrad, giving a small accuracy gain. For a typical Switchboard DNN with 46M parameters, we reach computation speeds of 27k frames per second (kfps) when using 2880 samples per minibatch, and 51kfps with 16k, on a server with 8 K20X GPUs. This corresponds to speed-ups over a single GPU of 3.6 and 6.3, respectively. 7 training passes over 309h of data complete in under 7h. A 160M-parameter model training processes 3300h of data in under 16h on 20 dual-GPU servers-a 10 times speed-up-albeit at a small accuracy loss.},
annote = {1-bit sgd
CD-DNN-HMM模型（语音）
用上一轮迭代的量化误差来补偿当前的局部梯度},
author = {Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
file = {:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Seide et al. - 2014 - 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs.pdf:pdf},
issn = {19909772},
pages = {1058--1062},
title = {{1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs}},
url = {https://www.isca-speech.org/archive/interspeech{\_}2014/i14{\_}1058.html},
year = {2014}
}
@techreport{Alistarh,
abstract = {Parallel implementations of stochastic gradient descent (SGD) have received significant research attention, thanks to its excellent scalability properties. A fundamental barrier when parallelizing SGD is the high bandwidth cost of communicating gradient updates between nodes; consequently, several lossy compresion heuristics have been proposed, by which nodes only communicate quantized gradients. Although effective in practice, these heuristics do not always converge. In this paper, we propose Quantized SGD (QSGD), a family of compression schemes with convergence guarantees and good practical performance. QSGD allows the user to smoothly trade off communication bandwidth and convergence time: nodes can adjust the number of bits sent per iteration, at the cost of possibly higher variance. We show that this trade-off is inherent, in the sense that improving it past some threshold would violate information-theoretic lower bounds. QSGD guarantees convergence for convex and non-convex objectives, under asynchrony, and can be extended to stochastic variance-reduced techniques. When applied to training deep neural networks for image classification and automated speech recognition, QSGD leads to significant reductions in end-to-end training time. For instance, on 16GPUs, we can train the ResNet-152 network to full accuracy on ImageNet 1.8× faster than the full-precision variant.},
archivePrefix = {arXiv},
arxivId = {1610.02132},
author = {Alistarh, Dan and Grubic, Demjan and Li, Jerry Z. and Tomioka, Ryota and Vojnovic, Milan},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1610.02132},
file = {:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alistarh et al. - 2017 - QSGD Communication-efficient SGD via gradient quantization and encoding.pdf:pdf},
issn = {10495258},
pages = {1710--1721},
title = {{QSGD: Communication-efficient SGD via gradient quantization and encoding}},
url = {http://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding},
volume = {2017-Decem},
year = {2017}
}
@article{Yang2019,
abstract = {Today's artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security.We propose a possible solution to these challenges: Secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning.We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.},
archivePrefix = {arXiv},
arxivId = {1902.04885},
author = {Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
doi = {10.1145/3298981},
eprint = {1902.04885},
file = {:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2019 - Federated Machine Learning Concept and Applications(4).pdf:pdf;:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2019 - Federated machine learning Concept and applications(5).pdf:pdf},
isbn = {10.1145/3298981},
issn = {21576912},
journal = {ACM Transactions on Intelligent Systems and Technology},
keywords = {Additional Key Words and Phrases: Federated learni,CCS Concepts: • Security and privacy,Federated learning,GDPR,Machine learning,Supervised learning,transfer learning,transfer learning ACM Reference format:,• Computing methodologies → Artificial intelligenc},
month = {jan},
number = {2},
publisher = {Association for Computing Machinery},
title = {{Federated machine learning: Concept and applications}},
url = {https://doi.org/10.1145/3298981},
volume = {10},
year = {2019}
}
@article{Sattler2019,
abstract = {Federated Learning allows multiple parties to jointly train a deep learning model on their combined data, without any of the participants having to reveal their local data to a centralized server. This form of privacy-preserving collaborative learning however comes at the cost of a significant communication overhead during training. To address this problem, several compression methods have been proposed in the distributed training literature that can reduce the amount of required communication by up to three orders of magnitude. These existing methods however are only of limited utility in the Federated Learning setting, as they either only compress the upstream communication from the clients to the server (leaving the downstream communication uncompressed) or only perform well under idealized conditions such as iid distribution of the client data, which typically can not be found in Federated Learning. In this work, we propose Sparse Ternary Compression (STC), a new compression framework that is specifically designed to meet the requirements of the Federated Learning environment. Our experiments on four different learning tasks demonstrate that STC distinctively outperforms Federated Averaging in common Federated Learning scenarios where clients either a) hold non-iid data, b) use small batch sizes during training, or where c) the number of clients is large and the participation rate in every communication round is low. We furthermore show that even if the clients hold iid data and use medium sized batches for training, STC still behaves pareto-superior to Federated Averaging in the sense that it achieves fixed target accuracies on our benchmarks within both fewer training iterations and a smaller communication budget.},
archivePrefix = {arXiv},
arxivId = {1903.02891},
author = {Sattler, Felix and Wiedemann, Simon and Muller, Klaus-Robert and Samek, Wojciech},
doi = {10.1109/tnnls.2019.2944481},
eprint = {1903.02891},
file = {:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sattler et al. - 2019 - Robust and Communication-Efficient Federated Learning From Non-i.i.d. Data.pdf:pdf},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
month = {nov},
pages = {1--14},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{Robust and Communication-Efficient Federated Learning From Non-i.i.d. Data}},
year = {2019}
}
@article{Konecny2016,
abstract = {Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.},
archivePrefix = {arXiv},
arxivId = {1610.05492},
author = {Kone{\v{c}}n{\'{y}}, Jakub and McMahan, H. Brendan and Yu, Felix X. and Richt{\'{a}}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
eprint = {1610.05492},
file = {:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kone{\v{c}}n{\'{y}} et al. - 2016 - Federated Learning Strategies for Improving Communication Efficiency.pdf:pdf},
journal = {arxiv.org},
title = {{Federated Learning: Strategies for Improving Communication Efficiency}},
url = {http://arxiv.org/abs/1610.05492},
year = {2016}
}
@techreport{Gupta,
abstract = {Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network's behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding.},
archivePrefix = {arXiv},
arxivId = {1502.02551},
author = {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
eprint = {1502.02551},
file = {:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gupta et al. - 2015 - Deep learning with limited numerical precision(2).pdf:pdf},
isbn = {9781510810587},
pages = {1737--1746},
title = {{Deep learning with limited numerical precision}},
url = {http://www.jmlr.org/proceedings/papers/v37/gupta15.pdf?source=post{\_}page---------------------------},
volume = {3},
year = {2015}
}
@techreport{Suresh2017,
abstract = {Motivated by the need for distributed learning and optimization algorithms with low communication cost, we study communication efficient algorithms for distributed mean estimation. Unlike previous works, we make no probabilistic assumptions on the data. We first show that for d dimensional data with n clients, a naive stochastic rounding approach yields a mean squared error (MSE) of Q(d/ti) and uses a constant number of bits per dimension per client. Wc then extend this naive algorithm in two ways: we show that applying a structured random rotation before quantization reduces the error to C((logd)/n) and a better coding strategy further reduces the error to 0(l/n). We also show that the latter coding strategy is optimal up to a constant in the minimax sense i.e., it achieves the best MSE for a given communication cost. We finally demonstrate the practicality of our algorithms by applying them to distributed Lloyd's algorithm for k-means and power iteration for PCA.},
archivePrefix = {arXiv},
arxivId = {1611.00429},
author = {Suresh, Ananda Theertha and Yu, Felix X. and Kumar, Sanjiv and McMahan, H. Brendan},
booktitle = {34th International Conference on Machine Learning, ICML 2017},
eprint = {1611.00429},
file = {:C$\backslash$:/Users/yemq3/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Suresh et al. - 2017 - Distributed mean estimation with limited communication(2).pdf:pdf},
isbn = {9781510855144},
pages = {5119--5128},
title = {{Distributed mean estimation with limited communication}},
url = {https://dl.acm.org/citation.cfm?id=3306025},
volume = {7},
year = {2017}
}
